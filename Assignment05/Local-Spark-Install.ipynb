{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this all about?\n",
    "\n",
    "In order to run Spark applications on your local machine, you must have **Java 8**, **Spark**, and the **PySpark** package installed.  Additionally, for the Jupyter Notebook kernel to use your locally installed **Spark**, you may need to use the **findspark** module.   If you are unsure whether you have any or all of these requirements, we recommend you follow the instructions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Java Installation\n",
    "\n",
    "Executing the following cell will install **Java 8** into your home directory (i.e., `$HOME`) in its own directory (i.e., `$HOME/java/`).  If you would rather install Java yourself, do not execute the following cell and download Java and follow the instructions for your operating system found on the [Java website](https://www.java.com/en/download/).\n",
    "\n",
    "*Note: If you have Java already installed, we recommend uninstalling it before running the following cell.  The next cell of code will delete anything located in `$HOME/java/` and add the installed Java directory to your `PATH` variable.  This installation code also assumes you use Bash as your default shell (and will modify your `$HOME/.bashrc` file).*\n",
    "\n",
    "**OS specific notes:**\n",
    "\n",
    "*Linux distros: This code has been tested and works for most distributions of Linux (32 and 64 bit)*\n",
    "\n",
    "*Mac OSX: The code to install Java may or may not work on your machine.  If you experience an error, please download Java and follow instructions for installation found on the [Java website](https://www.java.com/en/download/).*\n",
    "\n",
    "*Windows OS: You must download and follow instructions for installation found on the [Java website](https://www.java.com/en/download/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Java 8 installation!\n",
      "Now installing Java on Linux...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   909  100   909    0     0   5443      0 --:--:-- --:--:-- --:--:--  5443\n",
      "100 82.8M  100 82.8M    0     0  6720k      0  0:00:12  0:00:12 --:--:-- 6947kM    0     0  6558k      0  0:00:12  0:00:06  0:00:06 7132k\n",
      "Installation of Java 8 complete!\n"
     ]
    }
   ],
   "source": [
    "import platform as arch\n",
    "from sys import platform\n",
    "   \n",
    "print('Beginning Java 8 installation!')\n",
    "\n",
    "# Check which OS we are running on\n",
    "if platform.startswith('linux'):\n",
    "    print('Now installing Java on Linux...')\n",
    "    if arch.architecture()[0] == '64bit':\n",
    "        !curl -o ~/java.tar.gz -L https://javadl.oracle.com/webapps/download/AutoDL?BundleId=241526_1f5b5a70bf22433b84d0e960903adac8\n",
    "    elif arch.architecture()[0] == '32bit':\n",
    "        !curl -o ~/java.tar.gz -L https://javadl.oracle.com/webapps/download/AutoDL?BundleId=241524_1f5b5a70bf22433b84d0e960903adac8\n",
    "    !rm -rf ~/java && mkdir ~/java && tar -xzf ~/java.tar.gz -C ~/java --strip-components=1 && rm ~/java.tar.gz\n",
    "\n",
    "    # Define JAVA_HOME and add to PATH\n",
    "    !echo 'export JAVA_HOME=$HOME/java' >> ~/.bashrc\n",
    "    !echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    !. ~/.bashrc\n",
    "    \n",
    "    print('Installation of Java 8 complete!')\n",
    "\n",
    "elif platform == 'darwin':\n",
    "    print('Now installing Java on Mac...')\n",
    "    !curl -o ~/java.dmg -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234465_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    !hdiutil attach ~/java.dmg\n",
    "    !sudo installer -pkg /Volumes/Java\\ 8\\ Update\\ 181/Java\\ 8\\ Update\\ 181.app/Contents/Resources/JavaAppletPlugin.pkg -target /\n",
    "    !diskutil umount /Volumes/Java\\ 8\\ Update\\ 181 \n",
    "    print('Installation of Java 8 complete (maybe)... If there was an error, please mount java.dmg located in your home directory and follow the instructions to install')\n",
    "\n",
    "elif platform == 'win32':\n",
    "    print('You are running a Windows OS.  Please download the correct version of Java from here: https://java.com/en/download/manual.jsp and install following the instructions.')\n",
    "\n",
    "else:\n",
    "    print('We had trouble determining which OS you are running.  Please ask for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Spark Installation\n",
    "\n",
    "Executing the following cell will install the latest **Apache Spark** into your home directory (i.e., `$HOME`) in its own directory (i.e., `$HOME/spark/`).\n",
    "\n",
    "If you would rather install Spark yourself, do not execute the following cell and download the pre-built version of Spark found on the [Spark website](https://spark.apache.org/downloads.html).  You will need to extract the contents of the tarball and add Spark to your `PATH` variable.\n",
    "\n",
    "*Note: If you have Spark already installed, we recommend uninstalling it before running the following cell.  The next cell of code will delete anything located in `$HOME/spark/` and add the installed Spark directory to your `PATH` variable.  This installation code also assumes you use Bash as your default shell (and will modify your `$HOME/.bashrc` file).*\n",
    "\n",
    "**OS specific notes:**\n",
    "\n",
    "*Linux distros: This code has been tested and works for most distributions of Linux (32 and 64 bit).*\n",
    "\n",
    "*Mac OSX: The code has been tested and should work for modern Macs.*\n",
    "\n",
    "*Windows OS: You must download and follow instructions for installation found on the [Spark website](https://spark.apache.org/downloads.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Apache Spark installation!\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  219M  100  219M    0     0  1660k      0  0:02:15  0:02:15 --:--:-- 2020k0     0  1392k      0  0:02:41  0:00:37  0:02:04 2330k   0  0:02:20  0:01:55  0:00:25 85480\n",
      "Installation of Apache Spark complete!\n"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "\n",
    "print('Beginning Apache Spark installation!')\n",
    "\n",
    "# Check which OS we are running on\n",
    "if (platform.startswith('linux')) or (platform == 'darwin'):\n",
    "    # Download Spark and extract into $HOME/spark/\n",
    "    !curl -o ~/spark.tar.gz -L http://apache.cs.utah.edu/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "    !rm -rf ~/spark && mkdir ~/spark && tar -xzf ~/spark.tar.gz -C ~/spark --strip-components=1 && rm ~/spark.tar.gz\n",
    "    \n",
    "    # Define SPARK_HOME and add to PATH\n",
    "    !echo 'export SPARK_HOME=$HOME/spark' >> ~/.bashrc\n",
    "    !echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    \n",
    "    # Set spark master to localhost (may not be necessary)\n",
    "    !echo 'export SPARK_LOCAL_IP=\"127.0.0.1\"' >> ~/.bashrc\n",
    "    !. ~/.bashrc\n",
    "    \n",
    "    print('Installation of Apache Spark complete!')\n",
    "\n",
    "elif platform == 'win32':\n",
    "    print('You are running a Windows OS.  Please download the correct version of Spark from here: https://spark.apache.org/downloads.html and install following the instructions.')\n",
    "\n",
    "else:\n",
    "    print('We had trouble determining which OS you are running.  Please ask for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Pyspark Installation\n",
    "\n",
    "Executing the following cell will install **pyspark** and **findspark**.  We will use either `conda` (if you have anaconda3 or miniconda installed) or `pip`.  At the minimum, you must have `pip` installed.  You likely have one of these installed already!  If you do not, you can download and install [Anaconda3](https://www.anaconda.com/download/) or [pip](https://pip.pypa.io/en/stable/installing/).  We will also install **matplotlib** so you can plot results from your assignments and pre-build the font cache to save time in the future!\n",
    "\n",
    "*Note: These instructions should work regardless of the OS you are running!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages (20.0.2)\n",
      "Requirement already satisfied: pyspark in /home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages (from pyspark) (0.10.7)\n",
      "Collecting findspark\n",
      "  Using cached findspark-1.3.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.3.0\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 46 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 199 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting numpy>=1.11\n",
      "  Downloading numpy-1.18.1-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 97 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-2.4.6-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 1.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: setuptools in /home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0)\n",
      "Requirement already satisfied: six in /home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Installing collected packages: kiwisolver, cycler, numpy, pyparsing, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.3 numpy-1.18.1 pyparsing-2.4.6\n",
      "Python packages installation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigel/Documents/Courses/COSC526/spark_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Method to build Matplotlib font cache\n",
    "def buildFontCache():\n",
    "    import matplotlib\n",
    "    matplotlib.use('AGG')\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.plot([0],[0])\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "# Check for conda\n",
    "if shutil.which('conda'):\n",
    "    # Update conda\n",
    "    !conda update -n base conda --yes\n",
    "    \n",
    "    # Install pyspark and findspark\n",
    "    !conda install pyspark --yes\n",
    "    !conda install -c conda-forge findspark --yes\n",
    "    \n",
    "    # Install matplotlib and build font cache\n",
    "    !conda install matplotlib --yes\n",
    "    buildFontCache()\n",
    "    \n",
    "    print('Python package installation complete!')\n",
    "\n",
    "    \n",
    "# Check for pip if conda is not found\n",
    "elif shutil.which('pip'):\n",
    "    # Update pip\n",
    "    !pip install --upgrade pip\n",
    "    \n",
    "    # Install pyspark and findspark\n",
    "    !pip install pyspark\n",
    "    !pip install findspark\n",
    "    \n",
    "    # Install matplotlib and build font cache\n",
    "    !pip install matplotlib\n",
    "    buildFontCache()\n",
    "    \n",
    "    print('Python packages installation complete!')\n",
    "    \n",
    "else:\n",
    "    print('Could not find conda or pip, please follow the instructions above to install either Anaconda3 or pip.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it Work?\n",
    "\n",
    "Let's test that **Java**, **Spark**, **pyspark**, and **findspark** were all installed correctly.  The following should create a `SparkContext`, create an `RDD` from a python list, and print the values in the `RDD`.\n",
    "\n",
    "**Expected output:**\n",
    "`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "data = sc.parallelize(range(10))\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the output of the previous cell did not match the expected output or you received an error message, please ask for assistance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark Code\n",
    "\n",
    "Now that you have all the software installed to run **Spark** code in a Jupyter Notebook, keep in mind that you will need to use the following code at the beginning of each notebook where you wish to use **Spark**.  This initialization code will create a `SparkContext` which you can access via `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
