{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 526 - Assignment 09\n",
    "### March 19, 2021\n",
    "---\n",
    "\n",
    "In this notebook, we provide you with basic functions for completing the assignment.  *Complete the assignment in this notebook.  You will need to modify existing code and write new code to find a solution*.  Each member of the group must upload their own work (i.e., a notebook file) to GitHub.\n",
    "\n",
    "*Note: Running a cell will not rerun previous cells.  If you edit code in previous cells, you must rerun those cells.  If you are having trouble with undefined errors and code changes not applying, we recommend using* `Run All` *to avoid any errors results from not rerunning previous cells.  You can find this in the menu above:* `Cell -> Run All`\n",
    "\n",
    "Previously, we looked at food items reported in the dietary data from the National Health and Nutrition Examination Survey (NHANES).  As a reminder, NHANES is a cross-sectional survey that is conducted every two years in the United States. As part of the survey, individuals are asked to complete a demographics questionnaire and a 24-hour dietary recall. The 24-hour dietary recalls have been shown to be a valid and reliable method for describing usual dietary intakes of a population.  Given the tens of thousands of different foods an individual may report in the NHANES dietary recall data, it is beneficial to group (i.e., cluster) similar foods based on macronutrient and micronutrient content.  These food groups can be used to gain new insight into the dietary patterns of individuals and populations.  For example, if an individual eats only foods in the \"high fat, low carbohydrate\" group we may conclude that this individual is on the [ketogenic diet](https://en.wikipedia.org/wiki/Ketogenic_diet).\n",
    "\n",
    "Last assignment, we provided data with food items and fat and carbohdryate contents of each food item.  You may have visually noticed that many of the food clusters (found with $k$-Means) were very close and not very distinct (i.e., well separated).  In this assignment, we provide raw NHANES dietary intake sample data from 2 years of the survey.  These data include many more values for each food item, including micronutrient content (e.g., minerals and vitamins) and metadata (e.g., person reporting the food and time of reporting).  The inclusion of all nutrient content values increases the dimensionality of the data (i.e., the number of values representing each food item is large) and should also produce more distinct clusters.  Given the high-dimensionality of the data, it seems appropriate to use a clustering algorithm more complex than the $k$-Means we have used in previous assignments.\n",
    "\n",
    "In this assignment you will use [Density-based spatial clustering of applications with noise](https://en.wikipedia.org/wiki/DBSCAN) (DBSCAN) to cluster food items based on macronutrient and micronutrient content.  You learned about DBSCAN in today's lecture, but if you want to learn more, here is an [interactive DBSCAN model](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/).  We will reference the paper *[Development of a Scalable Method for Creating Food Groups Using the NHANES Dataset and MapReduce](https://dl.acm.org/citation.cfm?id=2975179)* (the ACM Digital Library is available through the University, but may not be available at home.  We have included the paper in this repo at `./paper.pdf`), which also used DBSCAN to cluster food items from the NHANES dataset.  You will use the data preprocessing code and the DBSCAN code provided in this paper's [git repo](https://github.com/TauferLab/NHANES-Analytics).\n",
    "\n",
    "*You will need to preprocess the raw NHANES dietary data (located at `./data/NHANES-20**-dietary.csv`), cluster it with DBSCAN, and reproduce figure 6 from the \"Development of a Scalable Method for Creating Food Groups Using the NHANES Dataset and MapReduce\" paper.  Note: your figures may be slightly different because you are only using 2 years of NHANES data, where the paper used 7 years of data.*\n",
    "\n",
    "We have isolated the relevant code from the paper, adapted it for this assignment, and included it in this repository.  If you would like to look at the source code, open the python files located in `./src/`, or visit the [git repo](https://github.com/TauferLab/NHANES-Analytics).  Below, we load a SparkContext and provide an example of how to load the NHANES dietary data, preprocess it, and cluster it.  Note that we also load a text file, `./data/features.txt` which contains the list of micro- and macronutrirent values we want from the raw NHANES data.  Use this code as a reference for how to use the provided preprocessing and clustering code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you are having trouble loading Spark, try uncommenting the following two lines\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Load code from paper\n",
    "from src.preprocess import cleanNHANESData\n",
    "from src.DBSCAN import DBSCAN\n",
    "\n",
    "# Load NHANES dietary data from 2009 and 2011\n",
    "# The preprocessing code expects a dictionary with RDDs of NHANES dietary data\n",
    "raw_data = {}\n",
    "for year in ['2009', '2011']:\n",
    "    raw_data[year] = sc.textFile('./data/NHANES-{}-dietary.csv'.format(year))\n",
    "    \n",
    "# Print total number of food item entries\n",
    "total = 0\n",
    "for year_data in raw_data.values():\n",
    "    total += year_data.count()\n",
    "print('Total number of food entries: {}'.format(total))\n",
    "    \n",
    "# Load list of nutrient features we want from the raw dietary data\n",
    "with open('./data/features.txt') as f:\n",
    "    features = f.readlines()\n",
    "features = [f.strip() for f in features]\n",
    "\n",
    "# Look at the list of features you will use for each food item\n",
    "# We need this list because some features (i.e., values) in the raw NHANES data cannot be used to cluster\n",
    "# Note that the first 4 features are:\n",
    "# \"USDA food code\", \"Respondent sequence number\", \"Modification code\", and \"Grams\"\n",
    "# The remaining features are micro- and macronutrient features\n",
    "# HINT: ref [1]\n",
    "print('List of features:\\n', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the first four features are not related to nutrient values.  These features are used in the cleaning process and will be removed post-processing. \n",
    "\n",
    "Apply the pre-processing to the data!  You will notice that there are tens of thousands food item entries.  We are only interested in the unique food items (e.g., if a person reports *bread* twice, we only care about one instance of *bread*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing code from paper\n",
    "# This may take a minute!\n",
    "clean_data = cleanNHANESData(sc, raw_data, features).persist()\n",
    "\n",
    "# Look at cleaned NHANES dietary data\n",
    "# Elements in RDD are key-value pairs of (food ID, np.array(nutrient value))\n",
    "print ('Total unique food items: {}'.format(clean_data.count()))\n",
    "print(clean_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data returned from `cleanNHANESData()` is an RDD of `(key, values)` where the keys are food item IDs (these uniquely identify each unique food item) and the values are micro- and macronutrient values!  Some of the nutrient values are negative.  This is because the nutrient values are [normalized and standardized](https://en.wikipedia.org/wiki/Normalization_%28statistics%29).  Now, apply the DBSCAN clustering below!  **This may take several minutes, depending on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DBSCAN parameters\n",
    "epsilon = 1\n",
    "min_pts = 4\n",
    "metric = 'euclidean'\n",
    "\n",
    "# Cluster the food items with DBSCAN\n",
    "# This may take a few minutes, depending on your machine!\n",
    "food_clusters = DBSCAN(sc, clean_data, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "\n",
    "# Look at the cluster results\n",
    "# Elements in the RDD are key-value pairs of (food ID, cluster ID)\n",
    "print('Clustered food items: {}'.format(food_clusters.count()))\n",
    "print(food_clusters.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting RDD from DBSCAN contain `(key, value)` pairs.  The `key` is the ID of the food item in the cluster.  The `value` is the cluster ID.  This version of DBSCAN labels clusters by the food item with the *minimum* ID value in a cluster.  Last, you will see that the number of food items clustered is smaller than the total number of unique food items because DBSCAN removes *noise* (i.e., food items that are not similar to other food items)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "\n",
    "Recreating Figure 6 from the paper should be accomplished in incremental steps.  Now that you know how to preprocess and cluster food items based on nutrient content using DBSCAN, you can develop metrics to report in the figure.  Figure 6 reports (1) the percentage of food items clustered and (2) the number of food clusters found.  *Define the functions below which intake food clusters (i.e., the output of DBSCAN) and returns percentage of food items clustered and number of food clusters found.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to report the percent of total food items that are clustered\n",
    "# The input 'clusters' should be the output of DBSCAN\n",
    "# The input 'total' should be the total number of unique food items (before clustering)\n",
    "# HINT: ref [2]\n",
    "def percentClustered(clusters, total):\n",
    "\n",
    "    \n",
    "# Define a function to count the number of clusters\n",
    "# The input 'clusters' should be the output of DBSCAN\n",
    "# HINT: ref [2,3]\n",
    "def clusterCount(clusters):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the functions you defined about to report the percent of food items clustered and the number of food clusters from the clusters you found above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions you defined to the clusters found above\n",
    "perc_clust = percentClustered(food_clusters, clean_data.count())\n",
    "clust_cnt = clusterCount(food_clusters)\n",
    "\n",
    "# Print results\n",
    "print('Food items clustered: {:.2f}%'.format(perc_clust))\n",
    "print('Number of food clusters: {}'.format(clust_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output:**\n",
    "\n",
    "```\n",
    "Food items clustered: 34.68%\n",
    "Number of food clusters: 79\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- [1: NHANES 2011 Nutrient Labels](https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DR1IFF_G.htm)\n",
    "- [2: RDD.count](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=count#pyspark.RDD.count)\n",
    "- [3: RDD.distinct](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=count#pyspark.RDD.distinct)\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "Now that you have a method to recreate the values found in Figures 6 of the paper, you must create a plotting script.  *Below, define a function that intakes a 2D array and plots a heatmap.*  We provide `dummy_data` for you to test your plotting method.  Remember, you should label the axes, provide the correct axes tick labels, and give the heatmap a title!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define a function that intake a 2D array of data values and plots a heatmap\n",
    "# Make sure you include axes and tick labels as well as a color bar!\n",
    "# HINT: ref [1,2,3]\n",
    "def plotHeatMap(data, x_title='X Axis', y_title='Y Axis', title='', x_ticks=[], y_ticks=[]):\n",
    "\n",
    "\n",
    "\n",
    "# Test plotting function with dummy data\n",
    "dummy_data = np.random.randint(20, size=(5,10))\n",
    "plotHeatMap(dummy_data, x_ticks=range(10), y_ticks=range(5), title='Dummy Data Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your heatmap should look similar to this:\n",
    "<img src=\"./sample_heatmap.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- [1: pyplot.pcolor](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolor.html)\n",
    "- [2: Making a heatmap with pcolor example](https://stackoverflow.com/questions/14391959/heatmap-in-matplotlib-with-pcolor)\n",
    "- [3: pyplot.colorbar](https://matplotlib.org/api/colorbar_api.html)\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3:\n",
    "\n",
    "Now that you have methods for calculating the values in Figure 6 from the paper and a method to plot the values, you can recreate the figure!  **To save time, you will recreate a figure similar to Figure 6 in the paper, but with only 30% of the the data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 30% of the original data\n",
    "clean_data_sample = clean_data.sample(False, 0.30).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the cell below, cluster food items with the euclidean distance metric and `epsilon` and `min_pts` values in ranges $[2,4]$ and $[4,7]$, respectively.  Plot two heatmaps.  One heatmap should show the percentage of food items clustered and the second heatmap should show the number of clusters found.*  Note: We use values for `epsilon` and `min_pts` that are different than those in the paper because we are using a smaller, less dense dataset.  The figures you make will be different than Figure 6 from the paper.  In the cell below, we provide two arrays.  Fill these arrays with the appropriate values and use them to create your heatmaps.  **This may take several minutes, depending on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distance metric we want to use\n",
    "metric = 'euclidean'\n",
    "\n",
    "# Test each combination of epsilon and minpts value in the ranges defined above!\n",
    "# Fill each of the numpy arrays below with the correct values for epsilon i and min_pts j\n",
    "# For example, when epsilon is 4 and min_pts is 5, enter values in array location [2,1]\n",
    "hm_per_data = np.zeros((3, 4))\n",
    "hm_count_data = np.zeros((3, 4))\n",
    "\n",
    "# Created nested for loops to enumerate each combination of epsilon and min_pts value\n",
    "# Record the percentage of food items clustered and the number of clusters\n",
    "# We reccommend you use print statements to see the progress!\n",
    "\n",
    "        \n",
    "# Plot the results in heatmaps\n",
    "plotHeatMap(hm_per_data, y_title='epsilon', x_title='min_pts',\\\n",
    "            title='Percent Food Items Clustered', y_ticks=range(2,5), x_ticks=range(4,8))\n",
    "plotHeatMap(hm_count_data, y_title='epsilon', x_title='min_pts',\\\n",
    "            title='Numer of Food Clusters', y_ticks=range(2,5), x_ticks=range(4,8))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4:\n",
    "\n",
    "While we cannot visualize the 40+ dimensions of nutrient data present, we can project the data (and clusters) down to 2 dimensions and visualize the clusters.  Select an `epsilon` and `min_pts` values that you think clusters the food items best (based on your heatmaps in Problem 3).  *Define values for `epsilon` and `min_pts` in the cell below and examine the resulting clusters.  Try different values for the DBSCAN input parameters and see how the clusters change!*  Below we provide code to cluster the data with DBSCAN and project the nutrient values to 2 Dimensions for visual assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Projects and plots high dimensional food clusters to a 2D axis\n",
    "# Input: clusters: RDD <<(cluster ID, [nutrient, values]), (cluster ID, [nutrient, values]), ...>>\n",
    "# Return: None (shows a plot)\n",
    "def plotFoodClusters2D(clusters):\n",
    "    # Get cluster IDS to color plotted clusters\n",
    "    cluster_ids = clusters.keys().collect()\n",
    "    unique_ids = np.unique(cluster_ids).tolist()\n",
    "    cluster_ids = [unique_ids.index(i) for i in cluster_ids]\n",
    "    \n",
    "    # Reduce nutrient values to 2 dimensions with TSNE\n",
    "    # HINT: ref [1,2]\n",
    "    nutrient_values = clusters.values().collect()\n",
    "    cluster_embedded = TSNE(n_components=2).fit_transform(nutrient_values)\n",
    "    \n",
    "    # Plot clusters\n",
    "    X = cluster_embedded[:,0]\n",
    "    Y = cluster_embedded[:,1]\n",
    "    plt.scatter(X, Y, c=cluster_ids, s=3)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "# Define your values for epsilon and min_pts\n",
    "#epsilon = \n",
    "#min_pts = \n",
    "metric = 'euclidean'\n",
    "\n",
    "# Cluster the food items with your parameters and DBSCAN\n",
    "food_clusters = DBSCAN(sc, clean_data_sample, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "\n",
    "# Combine cluster ID with nutrient value\n",
    "# RDD is now << (food ID, (cluster ID, [nutrient, values])), (food ID, (cluster ID, [nutrient, values])), ...>>\n",
    "cluster_data = food_clusters.join(clean_data_sample)\n",
    "\n",
    "# Isolate cluster ID and nutrient value (i.e., get rid of food item ID)\n",
    "# RDD should be <<(cluster ID, [nutrient, values]), (cluster ID, [nutrient, values]), ...>>\n",
    "cluster_nutrient = cluster_data.values()\n",
    "\n",
    "# Plot clusters in 2D projected space and report cluster information\n",
    "plotFoodClusters2D(cluster_nutrient)\n",
    "perc_clust = percentClustered(food_clusters, clean_data_sample.count())\n",
    "clust_cnt = clusterCount(food_clusters)\n",
    "print('{} Clusters found with {:.2f} % food items clustered'.format(clust_cnt, perc_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- [1: TSNE Algorithm](https://lvdmaaten.github.io/tsne/)\n",
    "- [2: sklearn's TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5:\n",
    "\n",
    "For this assignment and previous assignments we have been using [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), but this distance metric can suffer from the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).  Other distance metrics may be better suited for this high-dimensional NHANES food data, such as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).  Below, we provide code that clusters the food items with DBSCAN using the *Cosine Similarity* metric.  Examine the clusters found using DBSCAN and *Cosine Similarity*, then answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define values for epsilon and min_pts\n",
    "epsilon = 0.1\n",
    "min_pts = 4\n",
    "metric = 'cosine'\n",
    "\n",
    "# Cluster the food items with your parameters and DBSCAN\n",
    "food_clusters = DBSCAN(sc, clean_data_sample, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "\n",
    "# Combine cluster ID with nutrient value\n",
    "cluster_data = food_clusters.join(clean_data_sample)\n",
    "\n",
    "# Isolate cluster ID and nutrient value (i.e., get rid of food item ID)\n",
    "cluster_nutrient = cluster_data.values()\n",
    "\n",
    "# Plot clusters in 2D projected space and report cluster information\n",
    "plotFoodClusters2D(cluster_nutrient)\n",
    "perc_clust = percentClustered(food_clusters, clean_data_sample.count())\n",
    "clust_cnt = clusterCount(food_clusters)\n",
    "print('{} Clusters found with {:.2f} % food items clustered'.format(clust_cnt, perc_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the clusters you found during last assignment with $k$-means and only two macronutrients to the cluster you found with DBSCAN and all the macronutrients and micronutrients.  What differences do you notice?  Which do you think is better, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Problem 4, you chose an `epsilon` and `min_pts` value based on the heatmaps you created in Problem 3.  What was your motivation for choosing these values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this assignment, you clustered food items using DBSCAN with the Euclidean distance metric and Cosine Similarity.  What differences did you notice?  Which do you believe is better, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN can only find clusters of a certain density, based on the `epsilon` and `min_pts` value provided.  However, some datasets may contain clusters with different densities.  How could the DBSCAN algorithm be improved for data containing clusters of different densities?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Consider:\n",
    "Many machine learning tasks have labeled data which indicates if a model correctly predicts an outcome.  With clustering, there are typically no labels for the datasets.  How can we be sure that our clusters are good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Questions:\n",
    "**Answer the following questions, in a couple sentences each, in the cells provided below**\n",
    "* List the key tasks you accomplished during this assignment?\n",
    "* Describe the challenges you faced in addressing these tasks and how you overcame these challenges?\n",
    "* Did you work with other students on this assignment? If yes, how did you help them? How did they help you? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Your answers here*\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Define title, motivation, contributions, and results \n",
    "\n",
    "Complete this form at https://forms.gle/gg73noS52HWW29bW7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your any further feedback here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
